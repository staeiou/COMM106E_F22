{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas seaborn xlsxwriter openpyxl scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "We are using a new library called scikit-learn, originally created and released for free by researchers at the French national laboratory INRIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "__For lab: make a copy of [this sheet in Google Sheets](https://docs.google.com/spreadsheets/d/1IfMPD-PnYqS_fZiZVCElBz_BovOulMYj7cco8ssNf-I/edit?usp=sharing) and rename it with your PID (`A9999999_happysad`) in Google Sheets__\n",
    "\n",
    "\n",
    "\n",
    "__For lab: in Google Sheets, add bias for and/or against a particular group by adding at least 10 new rows__\n",
    "\n",
    "__For lab: download the spreadsheet as Excel and upload it to this folder. Make sure you rename the filename in the code cell below to match the one you downloaded and uploaded__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"A9999999_happysad.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['output'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering out neutral sentences\n",
    "\n",
    "Sometimes we only care about some of the cases or categories in our dataset. What if we want to only make a happy/sad classifier, and not a happy/neutral/sad classifier? We can use query, and ask it to return all casses where the output is not equal (`!=`) to `'neutral'`. Note that the entire query has to be inside a double quote, and then if we are asking it to compare text, the text has to be inside single quotes. If `output` was a number, we might instead do `data.query(\"output != 0\")`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.query(\"output != 'neutral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_happysad = data.query(\"output != 'neutral'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_happysad['output'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing datasets\n",
    "We want to randomly select 80% of this data to use to train the model, then use the remaining 20% to test how good the model is on examples it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, output_train, output_test = sklearn.model_selection.train_test_split(data_happysad['input'], \n",
    "                                                                                             data_happysad['output'], \n",
    "                                                                                             test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blank model from a three-part pipeline and then train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.pipeline.Pipeline([\n",
    "    ('vect', sklearn.feature_extraction.text.CountVectorizer()),\n",
    "    ('tfidf', sklearn.feature_extraction.text.TfidfTransformer()),\n",
    "    ('clf', sklearn.naive_bayes.MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the other 20% of data\n",
    "Remember the `input_test` dataset? We will use the `model.predict()` function to score those 1035 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predicted = model.predict(input_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `model.score()` by first inputting the 'true' labels, then the predictions. We get a percentage of the model's __accuracy__:\n",
    "\n",
    "$accuracy = \\frac{\\mbox{number of correct predictions}}{\\mbox{total number of items predicted}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(output_test, output_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The confusion matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(output_test, output_predicted, labels=model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=model.classes_)\n",
    "display.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## If you have more than 2 categories, it might be more useful to look at the classification report:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sklearn.metrics.classification_report(output_test, output_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For lab: auditing for a new kind of bias\n",
    "\n",
    "__For lab: replace these countries with those from the kind of group you introduced a bias for/against. Be sure to include examples where you did not introduce a bias!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "audit_categories = [\"France\", \"Germany\", \"the United States\", \"China\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__For lab: change the line `sample_text = \"Yay! I won a vacation to \" + example + \"!\"` so that it makes sense for the kind of bias you are testing against__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "for example in audit_categories:\n",
    "\n",
    "    sample_text = \"Yay! I won a vacation to \" + example + \"!\"\n",
    "    probability = model.predict_proba([sample_text])[0][0]\n",
    "    \n",
    "    result = {'example':example,\n",
    "              'happy_prediction':probability}\n",
    "    \n",
    "    results_list.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit = pd.DataFrame(results_list)\n",
    "data_audit.sort_values('happy_prediction')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Saving results of audit to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit.sort_values('happy_prediction').to_excel(\"audit_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b4d432cc475b37b45bc9f941b0f61c6e9f18b4be8b57c80de35b509106efdd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
