{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch torchvision torchaudio --extra-index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas seaborn xlsxwriter openpyxl transformers sentencepiece alive-progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "We are using a new library called scikit-learn, originally created and released for free by researchers at the French national laboratory INRIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can find new models at: https://huggingface.co/models?pipeline_tag=text-classification\n",
    "\n",
    "Click the \"Use in Transformers\" button near the top-right, and copy-paste the top box into the cell below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")\n",
    "\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"cardiffnlp/twitter-roberta-base-sentiment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.config.output_scores = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_prob(input_text, model):\n",
    "    with torch.no_grad():\n",
    "        inputs = tokenizer(input_text, return_tensors=\"pt\")\n",
    "        output = model(**inputs)\n",
    "        probs = output.logits.softmax(1)\n",
    "        return float(probs[0][0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_prob(\"I'm really sad\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_prob(\"I'm really happy\", model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_excel(\"COMM106E_happysad.xlsx\")\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# For each sentence in our original dataset, have the model score it\n",
    "\n",
    "This takes 5-15 seconds per score, so it takes a long time! We can use `alive_progress` library by "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from alive_progress import alive_bar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "with alive_bar(len(data), force_tty=True) as bar:\n",
    "    for index_num, row in data.iterrows():\n",
    "        prob_negative = score_prob(row['input'],model)\n",
    "        result = {'input' : row['input'],\n",
    "                  'true_label' : row['output'],\n",
    "                  'score' : prob_negative}\n",
    "\n",
    "        results_list.append(result)\n",
    "\n",
    "        print(result)\n",
    "        bar()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Auditing for country bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "countries = pd.read_csv(\"countries.csv\")\n",
    "countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_list = []\n",
    "\n",
    "with alive_bar(len(countries), force_tty=True) as bar:\n",
    "\n",
    "    for country_name in countries['country']:\n",
    "\n",
    "        sample_text = \"Yay! I won a vacation to \" + country_name + \"!\"\n",
    "        probability = score_prob(sample_text, model)\n",
    "\n",
    "        result = {'country':country_name,\n",
    "                  'prediction':probability}\n",
    "        print(result)\n",
    "        results_list.append(result)\n",
    "        bar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit = pd.DataFrame(results_list)\n",
    "data_audit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_audit.sort_values('prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b4d432cc475b37b45bc9f941b0f61c6e9f18b4be8b57c80de35b509106efdd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
