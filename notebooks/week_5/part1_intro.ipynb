{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install pandas seaborn xlsxwriter scikit-learn==1.0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries\n",
    "We are using a new library called scikit-learn, originally created and released for free by researchers at the French national laboratory INRIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sklearn.pipeline\n",
    "import sklearn.feature_extraction.text\n",
    "import sklearn.naive_bayes\n",
    "import sklearn.model_selection\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data\n",
    "We have a dataset of spam (junk email) and ham (good email). It also comes with a `label_num` where 0 is ham and 1 is spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\"spam_ham_dataset.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['output'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and testing datasets\n",
    "We want to randomly select 80% of this data to use to train the model, then use the remaining 20% to test how good the model is on examples it has not seen before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, input_test, output_train, output_test = sklearn.model_selection.train_test_split(data['input'], \n",
    "                                                                                             data['output'], \n",
    "                                                                                             test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_train, output_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test, output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a blank model from a three-part pipeline and then train it\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = sklearn.pipeline.Pipeline([\n",
    "    ('vect', sklearn.feature_extraction.text.CountVectorizer()),\n",
    "    ('tfidf', sklearn.feature_extraction.text.TfidfTransformer()),\n",
    "    ('clf', sklearn.naive_bayes.MultinomialNB()),\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(input_train, output_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test it on a few sentences in a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list = ['Subject: I need this report by 9am!',\n",
    "             'Subject: Buy peni$ pill$ 4 CHEAP',\n",
    "             'Subject: YOUR ACCOUNT HAS BEEN HACKED',\n",
    "             'Subject: Hi',\n",
    "             'Subject: Hi sexy']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions you can use to get a prediction. If you use `model.predict()`, then you just get the text of the predicted output label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict if each line of text is ham or spam\n",
    "model.predict(text_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use `model.predict_proba()` you get the probability percentage for all output labels. We only have two (ham and spam), but if you had more labels, it would show the probability for all of them. You can use `model.classes_` to get the labels in the order that `predict_proba()` displayes them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = model.predict_proba(text_list)\n",
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.classes_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_dataframe = pd.DataFrame(scores, columns=model.classes_)\n",
    "scores_dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing the model on the other 20% of data\n",
    "Remember the `input_test` dataset? We will use the `model.predict()` function to score those 1035 emails."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predicted = model.predict(input_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that the 'true' labels for these are in `output_test`. They are in a slightly different format (array displayed horizontally versus a column displayed horizontally), but they are easy for the computer to compare."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `model.score()` by first inputting the 'true' labels, then the predictions. We get a percentage of the model's __accuracy__:\n",
    "\n",
    "$accuracy = \\frac{\\mbox{number of correct predictions}}{\\mbox{total number of items predicted}}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.score(output_test, output_predicted))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Confusion Matrix\n",
    "\n",
    "But accruacy alone doesn't tell us everything: there could be way more false positives than false negatives. So we use the ___confusion matrix___, which is a 2x2 table of what kinds of correct vs incorrect predictions were made:\n",
    "\n",
    "![confusion matrix](https://indhumathychelliahcom.files.wordpress.com/2020/12/f653c-1x6gcmh3jedj_quso8pvl6q.png)\n",
    "\n",
    "[image from Indhumathy Chellia](https://indhumathychelliah.com/2020/12/23/confusion-matrix%E2%80%8A-%E2%80%8Aclearly-explained/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confusion_matrix = sklearn.metrics.confusion_matrix(output_test, output_predicted, labels=model.classes_)\n",
    "confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tp, fn, fp, tn = confusion_matrix.flatten()\n",
    "tp, fn, fp, tn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "display = sklearn.metrics.ConfusionMatrixDisplay(confusion_matrix, display_labels=model.classes_)\n",
    "display.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have no false negatives! We didn't have any cases where ham email was incorrectly predicted to be spam. But we have a lot of cases where spam email was incorrectly predicted to be ham -- about 40% of the time, it let spam through as ham."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metrics of success and failure\n",
    "\n",
    "Also look at the chart at https://en.wikipedia.org/wiki/Precision_and_recall#Definition_(classification_context)\n",
    "\n",
    "## ___Accuracy___:\n",
    "\n",
    "$\\frac{\\mbox{# correct predictions}}{\\mbox{# items predicted}}$\n",
    "\n",
    "## ___Precision___ or positive predictive value (PPV): \n",
    "\n",
    "$\\frac{\\mbox{# true positives}}{\\mbox{# true positives + # false positives}}$\n",
    "\n",
    "## ___Recall___, sensitivity, or  true positive rate (TPR):\n",
    "\n",
    "$\\frac{\\mbox{# true positives }}{\\mbox{# true positives + # false negatives}}$\n",
    "\n",
    "## ___Specificity___ or true negative rate (TNR):\n",
    "\n",
    "$\\frac{\\mbox{# true negatives }}{\\mbox{# true negatives + # false positives }}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Another visualization\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/422px-Precisionrecall.svg.png)\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/thumb/5/5a/Sensitivity_and_specificity_1.01.svg/422px-Sensitivity_and_specificity_1.01.svg.png)\n",
    "\n",
    "See https://enwp.org/Precision_and_recall and https://enwp.org/Sensitivity_and_specificity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = (tp + tn) / (tp + tn + fp + fn)\n",
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "precision = tp / (tp + fp)\n",
    "precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recall = tp / (tp + fn)\n",
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "specificity = tn / (tn + fp)\n",
    "specificity"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "97b4d432cc475b37b45bc9f941b0f61c6e9f18b4be8b57c80de35b509106efdd"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
